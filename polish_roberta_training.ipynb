{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoLitBert - Polish RoBERT'a model \n",
    "\n",
    "## Model training experiments' protocols.\n",
    "\n",
    "Training environment details:\n",
    "* Pytorch 1.5\n",
    "* Apex \n",
    "* CUDA 10.2\n",
    "* fairseq 0.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Experiments were additionally compared in a separate [research log](https://docs.google.com/spreadsheets/d/1fBhELqDB1kAxLCBvzeVM4OhqO4zx-meRUVljK1YZfF8/edit#gid=0)\n",
    "\n",
    "* Experiment 1 - linear decay, 50k updates\n",
    " * linear schedule peek_lr=5e-4  updates=50000, bsz=8192, test convergence speed of linear schedule, try to find optimal speed https://tensorboard.dev/experiment/pNCxXW9zQKKEoxkNN1LeKg/#scalars&tagFilter=lr%7C.*loss%24%7Cppl\n",
    "* Experiment 2 - cyclic triangular, 50k updates\n",
    " * cyclic triangular schedule updates=50000, bsz=8192, cyclic step=5000 test convergence speed of linear schedule, try to find optimal speed https://tensorboard.dev/experiment/pNCxXW9zQKKEoxkNN1LeKg/#scalars&tagFilter=lr%7C.*loss%24%7Cppl\n",
    "* Experiment 3 - cyclic cosine, 50k updates\n",
    " * TODO: EXPERIMENT_DESCRIPTION\n",
    "* Experiment 4 - cyclic cosine, 50k updates\n",
    " * cyclic cosine schedule,  updates=50000, bsz=8192, cyclic step=2500- test convergence speed of linear schedule, try to find optimal speed, should be similar to tirangular schedule in 5000 steps goes up and down with lr https://tensorboard.dev/experiment/SY64gY46SKq7wGohxgjlgg/#scalars&tagFilter=lr%7C.*loss%24%7Cppl after 23k steps experiment was stopped, loss jumped and plateau\n",
    "* Experiments 5, 6, 7 - cyclic cosine, 50k updates\n",
    " * TODO: EXPERIMENT_DESCRIPTION\n",
    "* Experiment 8 - cyclic triangular, 125k updates\n",
    " * TODO: EXPERIMENT_DESCRIPTION\n",
    "* Experiment 9 - cyclic cosine, 125k updates\n",
    " * TODO: EXPERIMENT_DESCRIPTION\n",
    "* Experiment 10 - linear, 125k updates\n",
    " * TODO: EXPERIMENT_DESCRIPTION\n",
    "* Experiment 11 - vocab50k, linear, 50k updates\n",
    " * TODO: EXPERIMENT_DESCRIPTION\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment 1 - linear decay, 50k updates\n",
    "\n",
    "Vocab: 32k tokens <br>\n",
    "Train on: AWS p3.16xlarge\n",
    "\n",
    "Efective batch size = MAX_SENTENCES\\*UPDATE_FREQ\\*num_gpu = 16\\*64\\*8 = 8192\n",
    "\n",
    "First experiment with linear scheduler, run for 50k updates.\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=50000     # Total number of training steps\n",
    "WARMUP_UPDATES=10000    # Warmup the learning rate over this many updates\n",
    "PEAK_LR=0.0005          # Peak learning rate, adjust as needed\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=64          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=./data/wiki_books_oscar/vocab32k/\n",
    "SAVE_DIR=./checkpoints/wiki_books_oscar_32k_linear/\n",
    "LOGS_DIR=./checkpoints/wiki_books_oscar_32k_linear/logs/\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1 --skip-invalid-size-inputs-valid-test \\\n",
    "    --save-dir $SAVE_DIR --tensorboard-logdir $LOGS_DIR --keep-last-epochs 10 \\\n",
    "    --ddp-backend=no_c10d\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "tensorboard dev upload --logdir $LOGS_DIR \\\n",
    "    --name \"PoLitBert - Polish RoBERT'a model, exp. #1\" \\\n",
    "    --description \"- linear decay, 50k updates, vocab32k, --save-dir ${SAVE_DIR}\"\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 - cyclic triangular, 50k updates\n",
    "\n",
    "Vocab: 32k tokens <br>\n",
    "Train on: AWS p3.16xlarge\n",
    "\n",
    "Efective batch size = MAX_SENTENCES\\*UPDATE_FREQ\\*num_gpu = 16\\*64\\*8 = 8192\n",
    "\n",
    "Cyclic triangular schedule, 5000 steps for rise to peek lr and fall to base lr,\n",
    "after each 5k steps shrink peak and base lr.\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=50000     # Total number of training steps\n",
    "STEP_SIZE=5000\n",
    "BASE_LR=0.0001\n",
    "PEAK_LR=0.001           # Peak learning rate, adjust as needed\n",
    "LR_SHRINK=0.8\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=64          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=./data/wiki_books_oscar/vocab32k/\n",
    "SAVE_DIR=./checkpoints/wiki_books_oscar_32k_tri/\n",
    "LOGS_DIR=./checkpoints/wiki_books_oscar_32k_tri/logs/\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler triangular --lr $BASE_LR --max-lr $PEAK_LR \\\n",
    "    --lr-period-updates $STEP_SIZE --lr-shrink $LR_SHRINK --shrink-min \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --skip-invalid-size-inputs-valid-test \\\n",
    "    --tensorboard-logdir $LOGS_DIR --log-format simple --log-interval 1  --save-dir $SAVE_DIR \\\n",
    "    --ddp-backend=no_c10d\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "tensorboard dev upload --logdir $LOGS_DIR \\\n",
    "    --name \"PoLitBert - Polish RoBERT'a model, exp. #2\" \\\n",
    "    --description \"- cyclic triangular, 50k updates, vocab32k, --save-dir ${SAVE_DIR}\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment 3 - cyclic cosine, 50k updates\n",
    "\n",
    "Vocab: upper 32k tokens <br>\n",
    "Train on: AWS p3.16xlarge\n",
    "\n",
    "Efective batch size = MAX_SENTENCES*UPDATE_FREQ*num_gpu = 16*64*8 = 8192\n",
    "\n",
    "Cyclic cosine schedule, 5000 steps for rise to peek lr and fall to base lr, after each 5k steps shrink\n",
    "peak and base lr.\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=50000     # Total number of training steps\n",
    "STEP_SIZE=2500\n",
    "WARMUP_UPDATES=2500     # same as triangular\n",
    "BASE_LR=0.0001\n",
    "PEAK_LR=0.001           # Peak learning rate, adjust as needed\n",
    "LR_SHRINK=0.8           #\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=64          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=./data/wiki_books_oscar/vocab32k/\n",
    "SAVE_DIR=./checkpoints/wiki_books_oscar_32k_cos1/\n",
    "LOGS_DIR=./checkpoints/wiki_books_oscar_32k_cos1/logs/\n",
    "\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler cosine --lr $BASE_LR --max-lr $PEAK_LR \\\n",
    "    --warmup-updates $WARMUP_UPDATES  \\\n",
    "    --lr-period-updates $STEP_SIZE --t-mult 1 --lr-shrink $LR_SHRINK  \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --skip-invalid-size-inputs-valid-test \\\n",
    "    --tensorboard-logdir $LOGS_DIR --log-format simple --log-interval 1  --save-dir $SAVE_DIR \\\n",
    "    --ddp-backend=no_c10d\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "tensorboard dev upload --logdir $LOGS_DIR \\\n",
    "    --name \"PoLitBert - Polish RoBERT'a model, exp. #3\" \\\n",
    "    --description \"- cyclic cosine, 50k updates, vocab32k, step=2500 --save-dir ${SAVE_DIR}\"\n",
    "\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4 - cyclic cosine, 50k updates\n",
    "\n",
    "Vocab: 32k tokens <br>\n",
    "Train on: AWS p3.16xlarge\n",
    "\n",
    "Effective batch size = MAX_SENTENCES\\*UPDATE_FREQ\\*num_gpu = 16\\*64\\*8 = 8192\n",
    "\n",
    "Cyclic cosine schedule, 5000 steps for rise to peek lr and fall to base lr,\n",
    "after each 5k steps shrink peak and base lr.\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=50000     # Total number of training steps\n",
    "STEP_SIZE=2500\n",
    "WARMUP_UPDATES=2500     # Same as triangular\n",
    "BASE_LR=0.0001\n",
    "PEAK_LR=0.0005          # Peak learning rate, adjust as needed\n",
    "LR_SHRINK=0.8\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=64          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=./data/wiki_books_oscar/vocab32k/\n",
    "SAVE_DIR=./checkpoints/wiki_books_oscar_32k_cos1_2/\n",
    "LOGS_DIR=./checkpoints/wiki_books_oscar_32k_cos1_2/logs/\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler cosine --lr $BASE_LR --max-lr $PEAK_LR \\\n",
    "    --warmup-updates $WARMUP_UPDATES  \\\n",
    "    --lr-period-updates $STEP_SIZE --t-mult 1  --lr-shrink $LR_SHRINK  \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --skip-invalid-size-inputs-valid-test \\\n",
    "    --tensorboard-logdir $LOGS_DIR --log-format simple --log-interval 1  --save-dir $SAVE_DIR \\\n",
    "    --ddp-backend=no_c10d --num-workers 2\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "tensorboard dev upload --logdir $LOGS_DIR \\\n",
    "    --name \"PoLitBert - Polish RoBERT'a model, exp. #4\" \\\n",
    "    --description \"- cyclic cosine, 50k updates, vocab32k, step=2500 half lr=0.0005, --save-dir ${SAVE_DIR}\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiments 5, 6, 7 - cyclic cosine, 50k updates\n",
    "\n",
    "Vocab: upper 32k tokens <br>\n",
    "Train on: AWS p3.16xlarge\n",
    "\n",
    "Efective batch size = MAX_SENTENCES*UPDATE_FREQ*num_gpu = 16*64*8 = 8192\n",
    "\n",
    "Cyclic cosine schedule, 1000 steps for rise to peek lr and fall to base lr.\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=50000     # Total number of training steps\n",
    "STEP_SIZE=1000\n",
    "WARMUP_UPDATES=1000\n",
    "BASE_LR=0.0001\n",
    "PEAK_LR=0.001           # Peak learning rate, adjust as needed\n",
    "LR_SHRINK=0.8\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=64          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=./data/wiki_books_oscar/vocab32k/\n",
    "SAVE_DIR=./checkpoints/wiki_books_oscar_32k_cos1_4/\n",
    "LOGS_DIR=./checkpoints/wiki_books_oscar_32k_cos1_4/logs/\n",
    "\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.9 \\\n",
    "    --lr-scheduler cosine --lr $BASE_LR --max-lr $PEAK_LR \\\n",
    "    --warmup-updates $WARMUP_UPDATES  \\\n",
    "    --lr-period-updates $STEP_SIZE --t-mult 2  --lr-shrink $LR_SHRINK  \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --skip-invalid-size-inputs-valid-test \\\n",
    "    --tensorboard-logdir $LOGS_DIR --log-format simple --log-interval 1  --save-dir $SAVE_DIR \\\n",
    "    --ddp-backend=no_c10d --num-workers 2\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "tensorboard dev upload --logdir $LOGS_DIR \\\n",
    "    --name \"PoLitBert - Polish RoBERT'a model, exp. #6\"  \\\n",
    "    --description \"- cyclic cosine, 50k updates, step=1000, t-mult=2, clip-norm=0.9, --save-dir ${SAVE_DIR}\"\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 8 - cyclic triangular, 125k updates\n",
    "\n",
    "Vocab: 32k tokens <br>\n",
    "Train on: AWS p3.16xlarge\n",
    "\n",
    "Efective batch size = MAX_SENTENCES\\*UPDATE_FREQ\\*num_gpu = 16\\*64\\*8 = 8192\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=125000    # Total number of training steps\n",
    "STEP_SIZE=5000\n",
    "BASE_LR=0.0001\n",
    "PEAK_LR=0.001           # Peak learning rate, adjust as needed\n",
    "LR_SHRINK=0.8\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=64          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=./data/wiki_books_oscar/vocab32k/\n",
    "SAVE_DIR=./checkpoints/wiki_books_oscar_32k_tri_full/\n",
    "LOGS_DIR=./checkpoints/wiki_books_oscar_32k_tri_full/logs/\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler triangular --lr $BASE_LR --max-lr $PEAK_LR \\\n",
    "    --lr-period-updates $STEP_SIZE --lr-shrink $LR_SHRINK --shrink-min \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --skip-invalid-size-inputs-valid-test \\\n",
    "    --tensorboard-logdir $LOGS_DIR --log-format simple --log-interval 1  --save-dir $SAVE_DIR \\\n",
    "    --ddp-backend=no_c10d\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "tensorboard dev upload --logdir $LOGS_DIR \\\n",
    "    --name \"PoLitBert - Polish RoBERT'a model, exp. #8\" \\\n",
    "    --description \"- cyclic triangular, 125k updates, vocab32k, --save-dir ${SAVE_DIR}\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 9 - cyclic cosine, 125k updates\n",
    "\n",
    "Vocab: upper 32k tokens <br>\n",
    "Train on: AWS p3.16xlarge\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=125000    # Total number of training steps\n",
    "STEP_SIZE=1000\n",
    "WARMUP_UPDATES=5000 \n",
    "BASE_LR=0.00001\n",
    "PEAK_LR=0.0007          # Peak learning rate, adjust as needed\n",
    "LR_SHRINK=0.7\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=64          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=./data/wiki_books_oscar/vocab32k/\n",
    "SAVE_DIR=./checkpoints/wiki_books_oscar_32k_cos1_5/\n",
    "LOGS_DIR=./checkpoints/wiki_books_oscar_32k_cos1_5/logs/\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.9 \\\n",
    "    --lr-scheduler cosine --lr $BASE_LR --max-lr $PEAK_LR \\\n",
    "    --warmup-updates $WARMUP_UPDATES  \\\n",
    "    --lr-period-updates $STEP_SIZE --t-mult 2  --lr-shrink $LR_SHRINK  \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --skip-invalid-size-inputs-valid-test \\\n",
    "    --tensorboard-logdir $LOGS_DIR --log-format simple --log-interval 1  --save-dir $SAVE_DIR \\\n",
    "    --ddp-backend=no_c10d --num-workers 2\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "tensorboard dev upload --logdir $LOGS_DIR \\\n",
    "    --name \"PoLitBert - Polish RoBERT'a model, exp. #9\"  \\\n",
    "    --description \"- cyclic cosine, 125k updates, vocab32k, t-mult=2 clip-norm=0.9, --save-dir ${SAVE_DIR}\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 10 - linear, 125k updates\n",
    "\n",
    "Vocab: 32k tokens <br>\n",
    "Train on: AWS p3.16xlarge\n",
    "\n",
    "Efective batch size = MAX_SENTENCES\\*UPDATE_FREQ\\*num_gpu = 16\\*64\\*8 = 8192\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=125000    # Total number of training steps\n",
    "WARMUP_UPDATES=10000    # Warmup the learning rate over this many updates\n",
    "PEAK_LR=0.001           # Peak learning rate, adjust as needed\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=64          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=./data/wiki_books_oscar/vocab32k/\n",
    "SAVE_DIR=./checkpoints/wiki_books_oscar_32k_linear_full/\n",
    "LOGS_DIR=./checkpoints/wiki_books_oscar_32k_linear_full/logs/\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.9 \\\n",
    "    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1 --skip-invalid-size-inputs-valid-test \\\n",
    "    --save-dir $SAVE_DIR --tensorboard-logdir $LOGS_DIR  \\\n",
    "    --ddp-backend=no_c10d\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "tensorboard dev upload --logdir $LOGS_DIR \\\n",
    "    --name \"PoLitBert - Polish RoBERT'a model, exp. #10 \"  \\\n",
    "    --description \"- linear, 125k updates, vocab32k, clip-norm=0.9, --save-dir ${SAVE_DIR}\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Experiment 11 - vocab50k, linear, 50k updates\n",
    "\n",
    "Vocab: 50k tokens\n",
    "Train on: AWS p3.16xlarge\n",
    "\n",
    "Efective batch size = MAX_SENTENCES\\*UPDATE_FREQ\\*num_gpu = 16\\*64\\*8 = 8192\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=50000     # Total number of training steps\n",
    "WARMUP_UPDATES=10000    # Warmup the learning rate over this many updates\n",
    "PEAK_LR=0.001           # Peak learning rate, adjust as needed\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=64          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=./data/wiki_books_oscar/vocab50k/\n",
    "SAVE_DIR=./checkpoints/wiki_books_oscar_50k_linear50k/\n",
    "LOGS_DIR=./checkpoints/wiki_books_oscar_50k_linear50k/logs/\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.9 \\\n",
    "    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1 --skip-invalid-size-inputs-valid-test \\\n",
    "    --save-dir $SAVE_DIR --tensorboard-logdir $LOGS_DIR  \\\n",
    "    --ddp-backend=no_c10d\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "tensorboard dev upload --logdir $LOGS_DIR \\\n",
    "    --name \"PoLitBert - Polish RoBERT'a model, exp. #11\"  \\\n",
    "    --description \"- linear, 50k updates, vocab50k, clip-norm=0.9, --save-dir ${SAVE_DIR}\"\n",
    "\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitherbertpipenvf409fddaf3f446fd8dcf7490c441f6bd",
   "display_name": "Python 3.7.3 64-bit ('herbert': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}