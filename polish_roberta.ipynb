{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitherbertpipenvbcec0f23f820467881614779f1f66588",
   "display_name": "Python 3.7.3 64-bit ('herbert': pipenv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-17ccbf5af75f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-17ccbf5af75f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Prepare the data.\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Herbert - Polish RoBERT'a model \n",
    "\n",
    "\n",
    "## Prepare the data.\n",
    "\n",
    "* Wikipedia, Link: \n",
    "* Oscar\n",
    "* Books, Polish \"Wolne lektury\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Get raw text from different sources and concat in one big data file. \n",
    "\n",
    "Usefull shell commands\n",
    "\n",
    "Cat all text files and instert new line between each text\n",
    "\n",
    "```sh\n",
    "find *.txt | xargs -I{} sh -c \"cat '{}'; echo ''\" > all_contents.txt\n",
    "\n",
    "```\n",
    "\n",
    "Take 11768022 first lines form splited wikipedia file\n",
    "```\n",
    "head -11768022 corpus_wiki_books_non_word_tokenized_2020-02-13.txt > corpus_wiki_lines_non_tok_2020-02-13.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "import\n"
    }
   ],
   "source": [
    "import csv\n",
    "import sys\n",
    "import datetime as dt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import tqdm as tq\n",
    "from tqdm.notebook import tqdm\n",
    "import mmap\n",
    "\n",
    "print(\"import\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils functions\n",
    "\n",
    "\n",
    "def get_num_lines(file_path):\n",
    "    fp = open(file_path, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(), 0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read wikipedia csv file and save in txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wikipedia_path='./data/wikipedia_2020-02-01.csv'\n",
    "output_path='./data/corpus_wikipedia_2020-02-01.txt'\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "t0=dt.datetime.now()\n",
    "with open(output_path, 'w+') as output_file:\n",
    "    with open(wikipedia_path) as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "        for article in csv_reader:\n",
    "            text = article['CleanText']\n",
    "\n",
    "            output_file.write(text)\n",
    "            output_file.write('\\n')   \n",
    "t1=dt.datetime.now()  \n",
    "print(f'Done. Takes={t1-t0}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Read book dataset and normalize line splitiing. The textfile has '\\n' in middle of the sentence. It is not necessary if you have proper file.\n",
    "\n",
    "Input: concatenated book textfile\n",
    "Output: file with removed new lines in the middle of the sentence. File has \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "data/corpus_books_2020_02_13\ndata/corpus_books_2020_02_13_norm.txt\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9438bfd1e66a46a185ecb6574838cc4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=58678005.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nDone. Takes=0:05:01.928267\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_path='./data/corpus_books_2020_02_13.txt'\n",
    "\n",
    "p = Path(input_path)\n",
    "print(p.with_suffix(''))\n",
    "output_path = f\"{p.with_suffix('')}_norm.txt\"\n",
    "\n",
    "print(output_path)\n",
    "\n",
    "# reg = re.compile('(?<!\\.)(\\n)(?=[a-zA-ZąćęłńóśźżĄĆĘŁŃÓŚŹŻ])',re.MULTILINE )\n",
    "# reg = re.compile('(?<!\\.)(\\n)(?=[a-ząćęłńóśźż])',re.MULTILINE )\n",
    "#reg = re.compile('(?<!\\.\\n)(\\n)(?=[a-ząćęłńóśźż])',re.MULTILINE )\n",
    "\n",
    "reg = re.compile('(?<=[A-Za-ząćęłńóśźż,—-])(?<!\\.)(\\n)(?=[a-ząćęłńóśźż])',re.MULTILINE )\n",
    "\n",
    "# https://pymotw.com/3/mmap/#regular-expressions\n",
    "\n",
    "line_buff = 101# 10007 # this is prime number, next is  10009  10037 \n",
    "N = 0 \n",
    "\n",
    "rep_str=' '\n",
    "t0=dt.datetime.now()\n",
    "with open(output_path, 'w+') as output_file:\n",
    "    with open(input_path) as f:\n",
    "\n",
    "        text=''\n",
    "        for line in tqdm(f,total=get_num_lines(input_path)):\n",
    "            # get block of file (line_buff) lines and replace \n",
    "            if N<line_buff:\n",
    "                # glue lines\n",
    "                text+=line\n",
    "                N+=1\n",
    "            else:\n",
    "                # proces and write\n",
    "                replace_text = reg.sub(rep_str, text)\n",
    "                output_file.write(replace_text)\n",
    "                text=''\n",
    "                N=0\n",
    "                \n",
    "        # for the rest of the file proces and write\n",
    "        if N>0:\n",
    "            replace_text = reg.sub(rep_str, text)\n",
    "            output_file.write(text)\n",
    "            \n",
    "\n",
    "t1=dt.datetime.now()  \n",
    "print(f'Done. Takes={t1-t0}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create vocabulary\n",
    "\n",
    "\n",
    "Create files for training vocabulary. One sentence per line, use polish sentence tokenizer. \n",
    "Sentence piece model is capable of handling around 12_000_000 sentences, so more sentences is not necessary.\n",
    "\n",
    "\n",
    "Vocab train sets:\n",
    "\n",
    "* Take 5M sentence from wiki, 7M from book corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n[nltk_data] Downloading package punkt to /home/ksopyla/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "./data/corpus_wikipedia_2020-02-01.txt\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e32b53688c540e8a3ac9e2ddab263ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=4355333.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "extra_abbreviations = ['ps',  'inc', 'Corp', 'Ltd', 'Co', 'pkt',  'Dz.Ap', 'Jr', 'jr' 'sp', 'Sp', 'poj',  'pseud', 'krypt', 'sygn', 'Dz.U', 'ws', 'itd', 'np', 'sanskryt', 'nr', 'gł', 'Takht', 'tzw', 't.zw', 'ewan', 'tyt', 'oryg' ]\n",
    "\n",
    "position_abbrev = ['Ks', 'Abp', 'abp','bp','dr', 'prof', 'zwycz', 'hab', 'arch', 'arch.kraj' 'B.Sc', 'Ph.D', 'lek', 'med', 'n.med' 'bł', 'św', 'hr' ]\n",
    "\n",
    "roman_abbrev= [] #['I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX', 'X', 'XI', 'XII', 'XII','XIV','XV','XVI', 'XVII', 'XVIII','XIX', 'XX', 'XXI' ]\n",
    "\n",
    "quantity_abbrev = [ 'mln', 'tys', 'obr./min' 'godz', 'egz'] #[ 'mln', 'tys', 'km/godz', 'obr./min' 'godz', 'egz']\n",
    "\n",
    "actions_abbrev = ['tłum','zob','wym', 'pot', 'ww', 'ogł', 'tzn', 'wyd', 'min', 'm.i', 'm.in', 'm. in' 'im','muz','tj', 'dot', 'wsp', 'właść', 'przedr', 'czyt', 'proj', 'dosł', 'hist' ]\n",
    "\n",
    "place_abbrev = ['Śl', 'płd']\n",
    "\n",
    "lang_abbrev = ['jęz','fr', 'ang', 'gr', 'hebr', 'czes', 'pol', 'niem', 'arab', 'egip', 'hiszp', 'jap', 'chin', 'kor', 'tyb', 'wiet']\n",
    "\n",
    "military_abbrev = ['kpt', 'kpr', 'obs', 'pil', 'mjr','płk', 'dypl', 'pp', 'gw', 'dyw', 'bryg', 'ppłk', 'mar', 'marsz' 'rez', 'ppor', 'DPanc', 'BPanc', 'DKaw']\n",
    "\n",
    "extra_abbreviations= extra_abbreviations + position_abbrev + roman_abbrev + quantity_abbrev + place_abbrev + actions_abbrev + place_abbrev + lang_abbrev+military_abbrev\n",
    "\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/polish.pickle')\n",
    "#sentence_tokenizer._params.abbrev_types.update(extra_abbreviations)\n",
    "\n",
    "#sentences = sentence_tokenizer.tokenize(line)\n",
    "\n",
    "files_to_proces = [\n",
    "    './data/corpus_wikipedia_2020-02-01.txt',\n",
    "    './data/corpus_books_2020_02_13.txt'\n",
    "]\n",
    "files_to_proces = [\n",
    "    './data/corpus_wikipedia_2020-02-01.txt',\n",
    "]\n",
    "# files_to_proces = [\n",
    "#     './data/corpus_books_2020_02_13.txt'\n",
    "# ]\n",
    "\n",
    "for input_file in files_to_proces:\n",
    "    print(input_file)\n",
    "    t0=dt.datetime.now()\n",
    "\n",
    "    p = Path(input_file)\n",
    "    output_path = f\"{p.with_suffix('')}_lines.txt\"\n",
    "\n",
    "    with open(output_path, 'w+') as output_file:\n",
    "        with open(input_file) as f:\n",
    "            for line in tqdm(f,total=get_num_lines(input_file)):\n",
    "\n",
    "                if line.strip() == '':\n",
    "                    output_file.write('\\n')\n",
    "                    continue;\n",
    "\n",
    "                sentences = sentence_tokenizer.tokenize(line)\n",
    "                \n",
    "                text = ''\n",
    "                for sentence in sentences:\n",
    "                    text += sentence.strip()\n",
    "                    text+='\\n'\n",
    "                output_file.write(text)\n",
    "                \n",
    "    t1=dt.datetime.now()  \n",
    "    print(f'Done. Takes={t1-t0}')   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the BPE sentencepiece model\n",
    "\n",
    "We used the [sentencepiece](https://github.com/google/sentencepiece)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 32000\n",
    "model_type = \"bpe\"  \n",
    "iss = 12_000_000\n",
    "\n",
    "lower_vocab = False\n",
    "txt_lower = \"lower\" if lower_vocab else \"upper\"\n",
    "tok_model = f\"wikibooks_{txt_lower}_voc_{max_vocab_size}_sent{iss}\"\n",
    "\n",
    "data_file = ''\n",
    "\n",
    "tok_model = os.path.abspath(f\"./data/{tok_model}\")\n",
    "\n",
    "piece_options = ' --bos_id=0 --eos_id=1 --pad_id=0 --unk_id=1 --shuffle_input_sentence=true'\n",
    "\n",
    "\n",
    "cmd = f\"--input={data_file} --model_prefix={tok_model} --model_type={model_type} --num_threads=4 --vocab_size={vocab_size}  --input_sentence_size={iss}\"+ piece_options\n",
    "print(cmd)\n",
    "\n",
    "\n"
   ]
  }
 ]
}