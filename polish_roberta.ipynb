{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitherbertpipenvf409fddaf3f446fd8dcf7490c441f6bd",
   "display_name": "Python 3.7.3 64-bit ('herbert': pipenv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-17ccbf5af75f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-17ccbf5af75f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Prepare the data.\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Herbert - Polish RoBERT'a model \n",
    "\n",
    "\n",
    "## Prepare the data.\n",
    "\n",
    "* Wikipedia, Link: \n",
    "* Oscar\n",
    "* Books, Polish \"Wolne lektury\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Get raw text from different sources and concat in one big data file. \n",
    "\n",
    "Usefull shell commands:\n",
    "\n",
    "\n",
    "Move files to another directory,  where isbn's are in the file list (wolne lektury non polish)\n",
    "\n",
    "```\n",
    "cat wolne_lektury_non_polish_isbn.txt | xargs -I{} sh -c \"mv *'{}'* ./non_polish/;\" \n",
    "```\n",
    "\n",
    "\n",
    "Cat all text files and instert new line between each text\n",
    "\n",
    "```sh\n",
    "find *content.txt | xargs -I{} sh -c \"cat '{}'; echo ''\" > corpus_[type]_[date].txt\n",
    "\n",
    "```\n",
    "\n",
    "Take 11768022 first lines form splited wikipedia file\n",
    "```\n",
    "head -11768022 corpus_wiki_books_non_word_tokenized_2020-02-13.txt > corpus_wiki_lines_non_tok_2020-02-13.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "import\n"
    }
   ],
   "source": [
    "import csv\n",
    "import sys\n",
    "import datetime as dt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import tqdm as tq\n",
    "from tqdm.notebook import tqdm\n",
    "import mmap\n",
    "\n",
    "print(\"import\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils functions\n",
    "\n",
    "\n",
    "def get_num_lines(file_path):\n",
    "    fp = open(file_path, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(), 0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read wikipedia csv file and save in txt.\n",
    "\n",
    "Run only once, just for generating file!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wikipedia_path='./data/wikipedia_2020-02-01.csv'\n",
    "output_path='./data/corpus_wikipedia_2020-02-01.txt'\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "t0=dt.datetime.now()\n",
    "with open(output_path, 'w+') as output_file:\n",
    "    with open(wikipedia_path) as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "        for article in csv_reader:\n",
    "            text = article['CleanText']\n",
    "\n",
    "            output_file.write(text)\n",
    "            # put new line of the end of the article\n",
    "            output_file.write('\\n')   \n",
    "t1=dt.datetime.now()  \n",
    "print(f'Done. Takes={t1-t0}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Read book dataset and normalize line splitiing. The textfile has '\\n' in middle of the sentence. It is not necessary if you have proper file.\n",
    "\n",
    "Input: concatenated book textfile\n",
    "Output: file with removed new lines in the middle of the sentence. \n",
    "\n",
    "Run once!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "data/corpus_books_2020_02_24\ndata/corpus_books_2020_02_24_fix.txt\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f1d79436754262860668f9610321cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=58596628.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nDone. Takes=0:11:21.175438\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_path='./data/corpus_books_2020_02_24.txt'\n",
    "\n",
    "p = Path(input_path)\n",
    "print(p.with_suffix(''))\n",
    "output_path = f\"{p.with_suffix('')}_fix.txt\"\n",
    "\n",
    "print(output_path)\n",
    "\n",
    "# reg = re.compile('(?<!\\.)(\\n)(?=[a-zA-ZąćęłńóśźżĄĆĘŁŃÓŚŹŻ])',re.MULTILINE )\n",
    "# reg = re.compile('(?<!\\.)(\\n)(?=[a-ząćęłńóśźż])',re.MULTILINE )\n",
    "#reg = re.compile('(?<!\\.\\n)(\\n)(?=[a-ząćęłńóśźż])',re.MULTILINE )\n",
    "\n",
    "\n",
    "# remove line breaks in the middle of the sentence\n",
    "reg = re.compile('(?<=[A-Za-ząćęłńóśźż,—-])(?<!\\.)(\\n)(?=[a-ząćęłńóśźż])',re.MULTILINE )\n",
    "#replace it by space\n",
    "rep_lines=' '\n",
    "\n",
    "# replace many dots in lines with one dot and line break\n",
    "#!!! bad regexp, removes spliting line between documents\n",
    "reg_dots = re.compile('(\\.*\\n){2,}',re.MULTILINE)\n",
    "rep_dots='.\\n'\n",
    "\n",
    "# https://pymotw.com/3/mmap/#regular-expressions\n",
    "\n",
    "line_buff = 10007# 10007 # this is prime number, next is  10009  10037 \n",
    "N = 0 \n",
    "\n",
    "def save_buffer2file(output_file, text):\n",
    "\n",
    "    # proces and write\n",
    "    replace_text = reg.sub(rep_lines, text)\n",
    "    #clean lines with only one character (dots, etc)\n",
    "    #replace_text = reg_dots.sub(rep_dots, replace_text)\n",
    "\n",
    "    output_file.write(replace_text)\n",
    "\n",
    "\n",
    "\n",
    "t0=dt.datetime.now()\n",
    "with open(output_path, 'w+') as output_file:\n",
    "    with open(input_path) as f:\n",
    "\n",
    "        text=''\n",
    "        for line in tqdm(f,total=get_num_lines(input_path)):\n",
    "            # get block of file (line_buff) lines and replace \n",
    "            if N<line_buff:\n",
    "                # glue lines\n",
    "                text+=line\n",
    "                N+=1\n",
    "            else:\n",
    "                save_buffer2file(output_file, text)\n",
    "                \n",
    "                text=''\n",
    "                N=0\n",
    "                \n",
    "        # for the rest of the file proces and write\n",
    "        if N>0:\n",
    "            save_buffer2file(output_file, text)\n",
    "            \n",
    "\n",
    "t1=dt.datetime.now()  \n",
    "print(f'Done. Takes={t1-t0}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create vocabulary\n",
    "\n",
    "\n",
    "### Prepare data for vocab\n",
    "\n",
    "Create files for training vocabulary. One sentence per line, use polish sentence tokenizer. \n",
    "Sentence piece model is capable of handling around 12_000_000 sentences, so more sentences is not necessary.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package punkt to /home/ksirg/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "#nltk.download()\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add necessary abbreviations to NLTK for wikipedia text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\ncurrent abbrev={'bie', 'e.s', 'mr', 'b.r', 'j.w', 'fot', 'f2', 'wł', 'mkw', 'pl', 'xiw', 'ż', 'j.c', 'th', 'b.b', '9x', 'dz', 'r.c', 'n.e', 'h.p', 'odl', 'rz', '2d', 'tad', 'gen', 'k.k', 'l', 'kl', 'o.n', 'sześc', 'wf', '1b', 'małg', 'adminem', 'r.m', 'gq', '4a', '2b', 'etc', 'inf', 'k.i.g', 'd.o.c', 'reż', 'k.m.b', 'ur', 'cenowo', 'doc', 'nt', 'hm', 'k', 'm.k', 'hab', 'ł.g', 'm.b', 'pocz', 'przejaśnieniami', 'tys', 's.t', 'ok', 'proc', 'w.w', 'inc', 'cyt', 'tzw', '13.i', 'offenbacha', 'v.f', 'm.in', 'a.ch', 'op', 'u.s', 'ad.5', 'kw', 'św', '7.b', 's', 'śp', 'd.k.p.c', 'p.o', 'głęb', 'szydercy-realisty', 'ad.4', 'str', 'l.r', '9.b', 'ms', 'maks', 'tzn', 'np', 'paszportów', 'błędem', 'rys', 's.a', 'tj', 'episkopat', 'mł', 'bm', 'ryc', 'j.ch', 'itp', 'w.m', 'pd', 'p.g', 'j.u', 'j', 'n.p.m', 'e.cz', 'ks', 'dł', 'k.p.c', 'dh', 'a.w', 'ds', 'inż', 'prof', 'wikipedii', 'st', 's3', '21.1.—18.2', 'ub.r', 'poł', 'lic', 'sz', 'l.w', '2c', 'mk', 'habeo', 's.c', 'z.z', 'pw', 'zach', 'b', 'br', '7.d', 'szt', 'm.t', 'ew', 'h.b', 'art', '1d', 'm.d', 'obw', 'dn', 'a.c', 'poz', 'os', 'j.k', '1971r', 'agd', 'p.n.e', 'h.s', 'p.e.n', 'p.s', 'g', 'ul', 'tow', 'sn', 'k.i', 'dol', '7.i', 'pt', 'j.a', 's.k', '9.i', 'cz', 'podst', 'f', 'szer', 'strawy', 'red', 'm.o', 'śr', 'woj', 'luf', 'm.st', 'por', 'a.l', 'a.k', '8.i', '2e', 'k.p', 's.o.s', 'n', 'c.s', 'm.a', 'w.h', 'ł.s', 'w.c', 'zn', 'płn', 'g.w', 'tab', 't', 'o.o', 'af', 'godz', '3d', 'nm', 'zw', 'ang', 'ś.p', 'dyr', 'zm', 'n.t', 'kaszb', 'wsch', 'hrubieszowie', 'j.s', 'łac', 'pow', 'jez', 'pn', 'a.s', 'g3', 'a.k.m', 'r', 'h', 'mec', 'ub', 'o.p', 'p.j', 'kpt', 'c.o', 'lit', 'z.d'}\n\nupdated abbrev={'bie', 'mr', 'alk', 'hr', 'pl', 'j.c', 'ż', 'th', 'wyd', 'czes', 'r.c', 'fr', 'hebr', 'h.p', 'odl', 'gr', 'abp', 'cal', 'tad', 'hiszp', 'j.m', 'Ks', 'gen', 'ppor', 'k.k', 'kl', 'sześc', 'wf', '1b', 'sum', 'med', 'Ltd', 'egz', 'pp', '4a', 'etc', 'dot', 'inf', 'm.i', 'reż', 'k.m.b', 'cenowo', 'doc', 'hm', 'marsz', 'm.k', 'ł.g', 'm.b', 'pocz', 's.t', 'tys', 'właśc', 'ok', 'muz', 'w.w', 'kpr', '13.i', 'v.f', 'm.in', 'DKaw', 'vs', 'Jr', 'u.s', 'św', 'głęb', 'ad.4', '9.b', 'maks', 'im', 'tzn', 'np', 'hist', 'Abp', 'tj', 'mł', 'bm', 'j.ch', 'pd', 'itp', 'p.g', 'w.m', 'geogr', 'e.cz', 'km/godz', 'ks', 'dł', 'k.p.c', 'kor', 'dh', 'Co', 'ppłk', 'inż', 'prof', 'wikipedii', 'st', '21.1.—18.2', 'ub.r', 'lic', 'sanskryt', 'bryg', 's.c', 'nr', 'zach', 'zwł', 'pw', '7.d', 'szt', 'arch.kraj', 'ew', 't.j', 'h.b', 'mar', 'm.d', 'a.c', 'poz', 'tyb', 'j.k', '1971r', 'Dz.Ap', 'bp', 'h.s', 'tow', 'płk', 'sn', 'Takht', 'dol', '7.i', 'pt', 'podst', 'f', 'szer', 'red', 'Corp', 'm.o', 'woj', 'Ph.D', 'wsp', 'egip', 'por', 'a.l', 'a.k', 'tlum', 'zob', '8.i', 'właść', 'mln', 'm.a', 'w.h', 'ł.s', 'zn', 'ewan', 'płn', 'przedr', 't.zw', 'tab', 't', '3d', 'af', 'zw', 'godz', 'ang', 'ś.p', 'n.t', 'hrubieszowie', 'pn', 'g3', 'a.k.m', 'pseud', 'mec', 'obs', 'ub', 'ukr', 'kpt', 'kard', 'c.o', 'z.d', 'wag', 'itd', 'bł', 'e.s', 'b.r', 'j.w', 'pkt', 'arch', 'fot', 'Dz.U', 'f2', 'wł', 'mkw', 'xiw', 'b.b', '9x', 'ps', 'dz', 'n.e', 'jap', 'rez', 'sp', 'rz', '2d', 'franc', 'pot', 'obr./min', 'dr', 'l', 'o.n', 'małg', 'adminem', 'r.m', 'gq', '2b', 'p.uł', 'k.i.g', 'd.o.c', 'ur', 'nt', 'poj', 'k', 'hab', 'ogł', 'przejaśnieniami', 'pol', 'proc', 'inc', 'zaw', 'cyt', 'tzw', 'offenbacha', 'BPanc', 'czyt', 'a.ch', 'op', 'ad.5', 'kw', '7.b', 'obj', 's', 'śp', 'd.k.p.c', 'p.o', 'szydercy-realisty', 'pil', 'mjr', 'tłum', 'str', 'chin', 'l.r', 'B.Sc', 'wiet', 'ms', 'ws', 'paszportów', 'błędem', 'rys', 's.a', 'episkopat', 'arab', 'j', 'ryc', 'ww', 'j.u', 'wym', 'n.p.m', 'a.w', 'jęz', 'ds', 'DPanc', 'l.poj', 'lek', 'dypl', 's3', 'krypt', 'poł', 'sz', 'l.w', 'z.z', '2c', 'mk', 'habeo', 'b', 'br', 'm.t', 'gł', 'mgr', 'ha', 'min', 'sygn', 'art', '1d', 'obw', 'daw', 'dn', 'tyt', 'oryg', 'os', 'agd', 'p.n.e', 'płd', 'p.e.n', 'p.s', 'g', 'ul', 'dyw', 'dziek', 'k.i', 'Sp', 'j.a', 's.k', '9.i', 'cz', 'strawy', 'śr', 'luf', 'm.st', 'niem', '2e', 'k.p', 'n', 's.o.s', 'dosł', 'c.s', 'w.c', 'g.w', 'proj', 'o.o', 'nm', 'dyr', 'zm', 'kaszb', 'wsch', 'j.s', 'łac', 'jr', 'jez', 'pow', 'a.s', 'r', 'n.med', 'gw', 'h', 'o.p', 'Śl', 'l.mn', 'p.j', 'zwycz', 'lit', 'in'}\nsymmetric diff= {'alk', 'hr', 'wyd', 'czes', 'fr', 'hebr', 'gr', 'abp', 'cal', 'hiszp', 'j.m', 'Ks', 'ppor', 'sum', 'med', 'Ltd', 'egz', 'pp', 'dot', 'm.i', 'marsz', 'właśc', 'muz', 'kpr', 'DKaw', 'vs', 'Jr', 'im', 'hist', 'Abp', 'geogr', 'km/godz', 'kor', 'Co', 'ppłk', 'sanskryt', 'bryg', 'nr', 'zwł', 'arch.kraj', 't.j', 'mar', 'tyb', 'Dz.Ap', 'bp', 'płk', 'Takht', 'Corp', 'Ph.D', 'wsp', 'egip', 'zob', 'tlum', 'właść', 'mln', 'ewan', 'przedr', 't.zw', 'pseud', 'obs', 'ukr', 'kard', 'wag', 'itd', 'bł', 'pkt', 'arch', 'Dz.U', 'ps', 'jap', 'rez', 'sp', 'franc', 'pot', 'obr./min', 'dr', 'p.uł', 'poj', 'ogł', 'pol', 'zaw', 'BPanc', 'czyt', 'obj', 'pil', 'mjr', 'tłum', 'chin', 'B.Sc', 'wiet', 'ws', 'arab', 'ww', 'wym', 'jęz', 'DPanc', 'l.poj', 'lek', 'dypl', 'krypt', 'gł', 'mgr', 'ha', 'min', 'sygn', 'daw', 'tyt', 'oryg', 'płd', 'dyw', 'dziek', 'Sp', 'niem', 'dosł', 'proj', 'jr', 'gw', 'n.med', 'Śl', 'l.mn', 'zwycz', 'in'}\nUpdatad diff= {'itd', 'bł', 'alk', 'pkt', 'hr', 'arch', 'Dz.U', 'wyd', 'ps', 'czes', 'fr', 'jap', 'hebr', 'rez', 'gr', 'sp', 'abp', 'cal', 'franc', 'hiszp', 'pot', 'j.m', 'Ks', 'ppor', 'obr./min', 'dr', 'sum', 'med', 'egz', 'pp', 'Ltd', 'dot', 'm.i', 'p.uł', 'marsz', 'poj', 'ogł', 'właśc', 'pol', 'muz', 'zaw', 'kpr', 'BPanc', 'czyt', 'DKaw', 'vs', 'Jr', 'obj', 'pil', 'mjr', 'tłum', 'chin', 'B.Sc', 'wiet', 'ws', 'im', 'hist', 'Abp', 'arab', 'ww', 'geogr', 'wym', 'km/godz', 'kor', 'Co', 'jęz', 'ppłk', 'DPanc', 'l.poj', 'lek', 'dypl', 'krypt', 'sanskryt', 'bryg', 'nr', 'zwł', 'arch.kraj', 'gł', 'mgr', 'ha', 'min', 't.j', 'sygn', 'mar', 'daw', 'tyt', 'oryg', 'tyb', 'Dz.Ap', 'bp', 'płd', 'dyw', 'płk', 'dziek', 'Takht', 'Sp', 'Corp', 'Ph.D', 'wsp', 'egip', 'niem', 'tlum', 'zob', 'właść', 'dosł', 'mln', 'ewan', 'przedr', 't.zw', 'proj', 'jr', 'gw', 'n.med', 'pseud', 'obs', 'Śl', 'l.mn', 'ukr', 'kard', 'zwycz', 'in', 'wag'}\nCurr diff= set()\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "extra_abbreviations = ['ps',  'inc', 'Corp', 'Ltd', 'Co', 'pkt', 'Dz.Ap', 'Jr', 'jr', 'sp', 'Sp', 'poj',  'pseud', 'krypt', 'sygn', 'Dz.U', 'ws', 'itd', 'np', 'sanskryt', 'nr', 'gł', 'Takht', 'tzw', 't.zw', 'ewan', 'tyt', 'oryg', 't.j', 'vs', 'l.mn', 'l.poj' ]\n",
    "\n",
    "position_abbrev = ['Ks', 'Abp', 'abp','bp','dr', 'kard', 'mgr', 'prof', 'zwycz', 'hab', 'arch', 'arch.kraj', 'B.Sc', 'Ph.D', 'lek', 'med', 'n.med', 'bł', 'św', 'hr', 'dziek' ]\n",
    "\n",
    "roman_abbrev= [] #['I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX', 'X', 'XI', 'XII', 'XII','XIV','XV','XVI', 'XVII', 'XVIII','XIX', 'XX', 'XXI' ]\n",
    "\n",
    "quantity_abbrev = [ 'mln', 'obr./min','km/godz', 'godz', 'egz', 'ha', 'j.m', 'cal', 'obj', 'alk', 'wag' ] # not added: tys.\n",
    "\n",
    "actions_abbrev = ['tłum','tlum','zob','wym', 'pot', 'ww', 'ogł', 'wyd', 'min', 'm.i', 'm.in', 'in', 'im','muz','tj', 'dot', 'wsp', 'właść', 'właśc', 'przedr', 'czyt', 'proj', 'dosł', 'hist', 'daw', 'zwł', 'zaw' ]\n",
    "\n",
    "place_abbrev = ['Śl', 'płd', 'geogr']\n",
    "\n",
    "lang_abbrev = ['jęz','fr','franc', 'ukr', 'ang', 'gr', 'hebr', 'czes', 'pol', 'niem', 'arab', 'egip', 'hiszp', 'jap', 'chin', 'kor', 'tyb', 'wiet', 'sum']\n",
    "\n",
    "military_abbrev = ['kpt', 'kpr', 'obs', 'pil', 'mjr','płk', 'dypl', 'pp', 'gw', 'dyw', 'bryg', 'ppłk', 'mar', 'marsz', 'rez', 'ppor', 'DPanc', 'BPanc', 'DKaw', 'p.uł']\n",
    "\n",
    "extra_abbreviations= extra_abbreviations + position_abbrev + roman_abbrev + quantity_abbrev + place_abbrev + actions_abbrev + place_abbrev + lang_abbrev+military_abbrev\n",
    "\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/polish.pickle')\n",
    "\n",
    "abbrev_set_curr = sentence_tokenizer._params.abbrev_types.copy()\n",
    "#update abbrev\n",
    "sentence_tokenizer._params.abbrev_types.update(extra_abbreviations)\n",
    "abbrev_set_upd = sentence_tokenizer._params.abbrev_types.copy()\n",
    "\n",
    "print(f'\\ncurrent abbrev={abbrev_set_curr}')\n",
    "sentence_tokenizer._params.abbrev_types.update(extra_abbreviations)\n",
    "print(f'\\nupdated abbrev={abbrev_set_upd}')\n",
    "\n",
    "print(f'symmetric diff= {abbrev_set_upd.symmetric_difference(abbrev_set_curr)}')\n",
    "\n",
    "print(f'Updatad diff= {abbrev_set_upd.difference(abbrev_set_curr)}')\n",
    "print(f'Curr diff= {abbrev_set_curr.difference(abbrev_set_upd)}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "./data/corpus_books_2020_02_24_fix.txt\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b942701641e45fcaf61a051893ae8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=35233221.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-93e1a1f56d05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/herbert-J41AiVc8/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \"\"\"\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/herbert-J41AiVc8/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \"\"\"\n\u001b[0;32m-> 1331\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/herbert-J41AiVc8/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \"\"\"\n\u001b[0;32m-> 1331\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/herbert-J41AiVc8/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/herbert-J41AiVc8/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[1;32m   1361\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/herbert-J41AiVc8/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/herbert-J41AiVc8/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1338\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'next_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/herbert-J41AiVc8/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtext_contains_sentbreak\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1381\u001b[0m         \"\"\"\n\u001b[1;32m   1382\u001b[0m         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# used to ignore last token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_annotate_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/herbert-J41AiVc8/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_annotate_second_pass\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m   1518\u001b[0m         \"\"\"\n\u001b[1;32m   1519\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_second_pass_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/herbert-J41AiVc8/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_second_pass_annotation\u001b[0;34m(self, aug_tok1, aug_tok2)\u001b[0m\n\u001b[1;32m   1546\u001b[0m         \u001b[0;31m# frequent sentence starters as their second word are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1547\u001b[0m         \u001b[0;31m# excluded in training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1548\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_typ\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollocations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1549\u001b[0m             \u001b[0maug_tok1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentbreak\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m             \u001b[0maug_tok1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabbr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "files_to_proces = [\n",
    "    './data/corpus_wikipedia_2020-02-01.txt',\n",
    "    './data/corpus_books_2020_02_24.txt'\n",
    "]\n",
    "files_to_proces = [\n",
    "    './data/corpus_wikipedia_2020-02-01.txt',\n",
    "]\n",
    "files_to_proces = [\n",
    "    './data/corpus_books_2020_02_24_fix.txt'\n",
    "]\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "for input_file in files_to_proces:\n",
    "    print(input_file)\n",
    "    t0=dt.datetime.now()\n",
    "\n",
    "    p = Path(input_file)\n",
    "    output_path = f\"{p.with_suffix('')}_lines.txt\"\n",
    "    text=''\n",
    "    with open(output_path, 'w+') as output_file:\n",
    "        with open(input_file) as f:\n",
    "            for line in tqdm(f,total=get_num_lines(input_file)):\n",
    "\n",
    "                # get block of text to new line which splits ariticles\n",
    "                text+=line\n",
    "                if line.strip() == '':\n",
    "\n",
    "                    sentences = sentence_tokenizer.tokenize(text)\n",
    "\n",
    "                    file_content = ''\n",
    "                    for sentence in sentences:\n",
    "                        file_content += sentence.strip()\n",
    "                        file_content+='\\n'\n",
    "                    output_file.write(file_content)\n",
    "                    \n",
    "                    output_file.write('\\n')\n",
    "                    text=''\n",
    "\n",
    "                ## old way\n",
    "                # if line.strip() == '':\n",
    "                #     output_file.write('\\n')\n",
    "                #     continue;\n",
    "\n",
    "                # sentences = sentence_tokenizer.tokenize(line)\n",
    "                \n",
    "                # text = ''\n",
    "                # for sentence in sentences:\n",
    "                #     text += sentence.strip()\n",
    "                #     text+='\\n'\n",
    "                # output_file.write(text)\n",
    "                \n",
    "    t1=dt.datetime.now()  \n",
    "    print(f'Done. Takes={t1-t0}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the BPE sentencepiece model\n",
    "\n",
    "We used the [sentencepiece](https://github.com/google/sentencepiece)\n",
    "\n",
    "We get: \n",
    "\n",
    "- 8 mln lines from wiki \n",
    "- 12 mln lines from books\n",
    "\n",
    "This can be done in two ways:\n",
    "- as a python module\n",
    "- as a command line \n",
    "\n",
    "\n",
    "If you want to use it as a commandline you should [build the C++ version from source](https://github.com/google/sentencepiece#c-from-source)\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "head -8M corpus_wikipedia_2020-02-01_lines.txt > corpus_wikipedia_2020-02-01_lines_8M.txt\n",
    "head -12M corpus_books_2020_02_13_fix_lines.txt > corpus_books_2020_02_13_fix_lines_12M.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train using commandline\n",
    "\n",
    "```\n",
    "spm_train \\\n",
    "    --input='./data/corpus_wikipedia_2020-02-01_lines.txt' \\\n",
    "    --max_sentence_length= [ max length of a sentence you accept ]\\\n",
    "    --model_prefix='.data/wikipedia.spm.bpe \\\n",
    "    --vocab_size=32000 \\\n",
    "    --model_type=bpe \\\n",
    "    --shuffle_input_sentence=true \\\n",
    "    --input_sentence_size=10000000 \\\n",
    "    --bos_id=0 --eos_id=1 --pad_id=2 --unk_id=3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 32000\n",
    "model_type = \"bpe\"  \n",
    "iss = 12_000_000\n",
    "\n",
    "lower_vocab = False\n",
    "txt_lower = \"lower\" if lower_vocab else \"upper\"\n",
    "tok_model = f\"wikibooks_{txt_lower}_voc_{max_vocab_size}_sent{iss}\"\n",
    "\n",
    "data_file = './data/corpus_wikipedia_2020-02-01_lines.txt'\n",
    "\n",
    "tok_model = os.path.abspath(f\"./data/{tok_model}\")\n",
    "\n",
    "piece_options = ' --bos_id=0 --eos_id=1 --pad_id=2 --unk_id=3 --shuffle_input_sentence=true'\n",
    "\n",
    "\n",
    "cmd = f\"--input={data_file} --model_prefix={tok_model} --model_type={model_type} --num_threads=4 --vocab_size={vocab_size}  --input_sentence_size={iss}\"+ piece_options\n",
    "print(cmd)\n",
    "\n",
    "\n"
   ]
  }
 ]
}