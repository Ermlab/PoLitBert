{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Herbert - Polish RoBERT'a model \n",
    "\n",
    "\n",
    "## Prepare the data.\n",
    "\n",
    "* Wikipedia, Link: \n",
    "* Oscar\n",
    "* Books, Polish \"Wolne lektury\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usefull resources\n",
    "\n",
    "\n",
    "* https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.pretraining.md\n",
    "* https://github.com/musixmatchresearch/umberto/issues/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "import\n"
    }
   ],
   "source": [
    "import csv\n",
    "import sys\n",
    "import datetime as dt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "#from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import mmap\n",
    "\n",
    "print(\"import\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary\n",
    "\n",
    "\n",
    "### Prepare data for vocab\n",
    "\n",
    "Create files for training vocabulary. One sentence per line, use polish sentence tokenizer. \n",
    "Sentence piece model is capable of handling around 12_000_000 sentences, so more sentences is not necessary.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "./data/corpus_books_2020_02_24_fix.txt\nin file=./data/corpus_books_2020_02_24_fix.txt\nout file=data/corpus_books_2020_02_24_fix_lines.txt\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=35233221.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c5940fbbde0a408cb19a906abd0f8d29"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "files_to_proces = [\n",
    "    './data/corpus_wikipedia_2020-02-01.txt',\n",
    "    './data/corpus_books_2020_02_24.txt'\n",
    "]\n",
    "files_to_proces = [\n",
    "    './data/corpus_wikipedia_2020-02-01.txt',\n",
    "]\n",
    "files_to_proces = [\n",
    "    './data/corpus_books_2020_02_24_fix.txt'\n",
    "]\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "for input_file in files_to_proces:\n",
    "    print(input_file)\n",
    "    p = Path(input_file)\n",
    "    output_path = f\"{p.with_suffix('')}_lines.txt\"\n",
    "\n",
    "    print(f\"in file={input_file}\\nout file={output_path}\")\n",
    "\n",
    "    t0=dt.datetime.now()\n",
    "\n",
    "    total_lines = get_num_lines(input_file)\n",
    "    \n",
    "    text=''\n",
    "    with open(output_path, 'w+') as output_file:\n",
    "        with open(input_file) as f:\n",
    "            i=0\n",
    "            for line in tqdm(f,total=total_lines):\n",
    "\n",
    "                # get block of text to new line which splits ariticles\n",
    "                text+=line\n",
    "                i+=1\n",
    "                if line.strip() == '' | i%100==0:\n",
    "\n",
    "                    sentences = sentence_tokenizer.tokenize(text)\n",
    "\n",
    "                    file_content = ''\n",
    "                    for sentence in sentences:\n",
    "                        file_content += sentence.strip()\n",
    "                        file_content+='\\n'\n",
    "                    output_file.write(file_content)\n",
    "                    \n",
    "                    output_file.write('\\n')\n",
    "                    text=''\n",
    "                    print(f'{i}/{total_lines}={i/total_lines}')\n",
    "\n",
    "                ## old way\n",
    "                # if line.strip() == '':\n",
    "                #     output_file.write('\\n')\n",
    "                #     continue;\n",
    "\n",
    "                # sentences = sentence_tokenizer.tokenize(line)\n",
    "                \n",
    "                # text = ''\n",
    "                # for sentence in sentences:\n",
    "                #     text += sentence.strip()\n",
    "                #     text+='\\n'\n",
    "                # output_file.write(text)\n",
    "                \n",
    "    t1=dt.datetime.now()  \n",
    "    print(f'Split lines done, takes={t1-t0}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the BPE sentencepiece model\n",
    "\n",
    "We used the [sentencepiece](https://github.com/google/sentencepiece)\n",
    "\n",
    "\n",
    "This can be done in two ways:\n",
    "- as a python module\n",
    "- as a command line \n",
    "\n",
    "\n",
    "If you want to use it as a commandline you should [build the C++ version from source](https://github.com/google/sentencepiece#c-from-source)\n",
    "\n",
    "\n",
    "Lower txt file \n",
    "\n",
    "```\n",
    "tr [:upper:] [:lower:] < corpus_books_wiki_12M_lines.txt > corpus_books_wiki_12M_lines_lower.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Train wikipedia vocab using commandline\n",
    "\n",
    "```\n",
    "spm_train \\\n",
    "    --input=./data/corpus_wikipedia_2020-02-01_lines.txt \\\n",
    "    --max_sentence_length=4192\\\n",
    "    --model_prefix=./data/wikipedia_v32k_sen10M.spm.bpe \\\n",
    "    --vocab_size=32000 \\\n",
    "    --model_type=bpe \\\n",
    "    --shuffle_input_sentence=true \\\n",
    "    --input_sentence_size=10000000 \\\n",
    "    --bos_id=0 --eos_id=1 --pad_id=2 --unk_id=3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "spm_train \\\n",
    "    --input=corpus_books_wiki_12M_lines_lower.txt \\\n",
    "    --max_sentence_length=4192\\\n",
    "    --model_prefix=books_wikipedia_lower_v32k_sen10M.spm.bpe \\\n",
    "    --vocab_size=32000 \\\n",
    "    --model_type=bpe \\\n",
    "    --shuffle_input_sentence=true \\\n",
    "    --input_sentence_size=10000000 \\\n",
    "    --bos_id=0 --eos_id=1 --pad_id=2 --unk_id=3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--input=./data/corpus_wikipedia_2020-02-01_lines.txt --model_prefix=/home/ksopyla/dev/ermlab/herbert/data/wikipedia_upper_voc_32000_sen10000000 --model_type=bpe --num_threads=4 --vocab_size=32000  --input_sentence_size=10000000 --bos_id=0 --eos_id=1 --pad_id=2 --unk_id=3 --shuffle_input_sentence=true\n2020-02-27 13:11:27.769435\nVocab of 32000 tokens from ./data/corpus_wikipedia_2020-02-01_lines.txt create takes 0:13:14.934491\n"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "vocab_size = 32000\n",
    "model_type = \"bpe\"  \n",
    "iss = 10_000_000\n",
    "\n",
    "vocab_type = 'upper' \n",
    "#vocab_type = 'lower' \n",
    "\n",
    "\n",
    "tok_model = f\"wikipedia_{vocab_type}_voc_{vocab_size}_sen{iss}\"\n",
    "\n",
    "data_file = './data/corpus_wikipedia_2020-02-01_lines.txt'\n",
    "\n",
    "tok_model = os.path.abspath(f\"./data/{tok_model}\")\n",
    "\n",
    "piece_options = ' --bos_id=0 --eos_id=1 --pad_id=2 --unk_id=3 --shuffle_input_sentence=true'\n",
    "\n",
    "\n",
    "cmd = f\"--input={data_file} --model_prefix={tok_model} --model_type={model_type} --num_threads=4 --vocab_size={vocab_size}  --input_sentence_size={iss}\"+ piece_options\n",
    "print(cmd)\n",
    "\n",
    "start = dt.datetime.now()\n",
    "print(start)\n",
    "spm.SentencePieceTrainer.train(cmd)\n",
    "end = dt.datetime.now()\n",
    "\n",
    "print(f\"Vocab of {vocab_size} tokens from {data_file} create takes {end-start}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "32000\n['▁Będąc', '▁młodym', '▁programi', 'stą', '▁(', 'ho', 'ho', '),', '▁czy', 't', 'ałem', '▁\"', 'D', 'zia', 'dy', '\"', '▁w', '▁1983', 'r', '.']\n"
    }
   ],
   "source": [
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f\"{tok_model}.model\")\n",
    "\n",
    "# returns vocab size\n",
    "print(sp.get_piece_size())\n",
    "text = \"\"\"Będąc młodym programistą (hoho), czytałem \"Dziady\" w 1983r.\"\"\"\n",
    "\n",
    "# encode: text => id\n",
    "if vocab_type==\"lower\":\n",
    "    text = text.lower()\n",
    "\n",
    "print(sp.encode_as_pieces(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the bpe sentecepiece vocab separator form \\t to space. It is format needed by fairseq-preprocess \n",
    "Open vocab file and replce \\t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "vocab_file = './data/wikipedia_upper_voc_32000_sen10000000.vocab'\n",
    "\n",
    "\n",
    "p = Path(vocab_file)\n",
    "\n",
    "output_path = f\"{p.with_suffix('')}_fair.vocab\"\n",
    "with open(output_path, 'w+') as output_file:\n",
    "    with open(vocab_file) as f:\n",
    "        \n",
    "        text = f.read().replace('\\t',' ')\n",
    "        output_file.write(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split corpus into train, valid, test datasets\n",
    "\n",
    "\n",
    "Split **corpus_wikipedia_2020-02-01.txt**\n",
    "\n",
    "* train - 0 -3500002\n",
    "* valid - 3500002 - 3950001\n",
    "* test - last - 3950001 - 4355333\n",
    "\n",
    "```\n",
    "head -3500002 corpus_wikipedia_2020-02-01.txt > wiki_model/corpus_wikipedia_2020-02-01_train.txt\n",
    "tail -$((4355333-3950001))  corpus_wikipedia_2020-02-01.txt > wiki_model/corpus_wikipedia_2020-02-01_test.txt\n",
    "\n",
    "head -3950001 corpus_wikipedia_2020-02-01.txt > corpus_wikipedia_2020-02-01_valid_part.txt\n",
    "tail -$((3950001-3500002))  corpus_wikipedia_2020-02-01_valid_part.txt > wiki_model/corpus_wikipedia_2020-02-01_valid.txt\n",
    "rm corpus_wikipedia_2020-02-01_valid_part.txt\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data with sentence piece model\n",
    "\n",
    "\n",
    "Encode wiki \n",
    "```\n",
    "# Encode Data with SentencePiece Tokenizer\n",
    "cd data/wiki_model\n",
    "\n",
    "for SPLIT in train valid test; do \\\n",
    "    spm_encode \\\n",
    "    --model=./vocab/wikipedia_upper_voc_32000_sen10000000.model \\\n",
    "    --extra_options=bos:eos \\\n",
    "    --output_format=piece \\\n",
    "    < corpus_wikipedia_2020-02-01_${SPLIT}.txt \\\n",
    "    > corpus_wikipedia_2020-02-01_${SPLIT}.txt.bpe\n",
    "done\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data with fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\u001b[39m\u001b[1mInstalling \u001b[32m\u001b[1mfairseq\u001b[39m\u001b[22m...\u001b[39m\u001b[22m\n\u001b[K\u001b[39m\u001b[1mAdding\u001b[39m\u001b[22m \u001b[32m\u001b[1mfairseq\u001b[39m\u001b[22m \u001b[39m\u001b[1mto Pipfile's\u001b[39m\u001b[22m \u001b[31m\u001b[1m[packages]\u001b[39m\u001b[22m\u001b[39m\u001b[1m...\u001b[39m\u001b[22m\n\u001b[K\u001b[?25h✔ Installation Succeeded\u001b[0m \n\u001b[39m\u001b[1mInstalling dependencies from Pipfile.lock (d640ed)...\u001b[39m\u001b[22m\n\n\u001b[0m"
    }
   ],
   "source": [
    "\n",
    "!pipenv install fairseq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairseq wiki preprocess bpe encoded and splited data\n",
    "\n",
    "```\n",
    "fairseq-preprocess \\\n",
    "    --only-source \\\n",
    "    --srcdict ./vocab/wikipedia_upper_voc_32000_sen10000000_fair.vocab \\\n",
    "    --trainpref corpus_wikipedia_2020-02-01_train.txt.bpe \\\n",
    "    --validpref corpus_wikipedia_2020-02-01_valid.txt.bpe \\\n",
    "    --testpref corpus_wikipedia_2020-02-01_test.txt.bpe \\\n",
    "    --destdir ./ \\\n",
    "    --workers 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS production settings\n",
    "\n",
    "#### p3 instance - linear decay\n",
    "\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=125000    # Total number of training steps\n",
    "WARMUP_UPDATES=10000    # Warmup the learning rate over this many updates\n",
    "PEAK_LR=0.0005          # Peak learning rate, adjust as needed\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=16          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=/mnt/efs/fs1/bert_model/datasets/roberta_data/wiki_model/\n",
    "SAVE_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_model\n",
    "LOGS_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_model/logs/\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1 --skip-invalid-size-inputs-valid-test \\\n",
    "    --save-dir $SAVE_DIR --tensorboard-logdir $LOGS_DIR --keep-last-epochs 10\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### g4 instance - wiki triangular\n",
    "\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=125000    # Total number of training steps\n",
    "STEP_SIZE=5000\n",
    "BASE_LR=0.0001\n",
    "PEAK_LR=0.001          # Peak learning rate, adjust as needed\n",
    "LR_SHRINK=0.6           # better is 0.9\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=16          # Increase the batch size 16x\n",
    "\n",
    "\n",
    "DATA_DIR=/mnt/efs/fs1/bert_model/datasets/roberta_data/wiki_model/\n",
    "SAVE_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_model_tri\n",
    "LOGS_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_model_tri/logs/\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler triangular --lr $BASE_LR --max-lr $PEAK_LR \\\n",
    "    --lr-period-updates $STEP_SIZE --lr-shrink $LR_SHRINK --shrink-min \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --skip-invalid-size-inputs-valid-test \\\n",
    "    --tensorboard-logdir $LOGS_DIR --log-format simple --log-interval 1  --save-dir $SAVE_DIR \\\n",
    "    --ddp-backend=no_c10d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### g4 instance - wiki cosine\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=30000    # Total number of training steps\n",
    "WARMUP_UPDATES=1000\n",
    "BASE_LR=0.00005\n",
    "PEAK_LR=0.0005          # Peak learning rate, adjust as needed\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=16          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=/mnt/efs/fs1/bert_model/datasets/roberta_data/wiki_model/\n",
    "SAVE_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_model_cos\n",
    "LOGS_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_model_cos/logs/\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler cosine --warmup-updates $WARMUP_UPDATES --lr-period-updates 1000 --t-mult 2 --lr-shrink 0.9 --lr $BASE_LR --max-lr $PEAK_LR \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --skip-invalid-size-inputs-valid-test \\\n",
    "    --tensorboard-logdir $LOGS_DIR --log-format simple --log-interval 1  --save-dir $SAVE_DIR\\\n",
    "    --ddp-backend=no_c10d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odra settings\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=30000    # Total number of training steps\n",
    "WARMUP_UPDATES=10000    # Warmup the learning rate over this many updates\n",
    "PEAK_LR=0.0005          # Peak learning rate, adjust as needed\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=6        \n",
    "UPDATE_FREQ=16          \n",
    "\n",
    "DATA_DIR=/mnt/hdd/goodwrite_nas/bert_model/datasets/roberta_data/wiki_model\n",
    "\n",
    "fairseq-train $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1 --skip-invalid-size-inputs-valid-test --tensorboard-logdir ./logs \n",
    "\n",
    "\n",
    "TOTAL_UPDATES=50000    # Total number of training steps\n",
    "WARMUP_UPDATES=5000\n",
    "PEAK_LR=0.001          # Peak learning rate, adjust as needed\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=6        \n",
    "UPDATE_FREQ=16          \n",
    "\n",
    "DATA_DIR=/mnt/hdd/goodwrite_nas/bert_model/datasets/roberta_data/wiki_model\n",
    "\n",
    "\n",
    "#after 32184 steps, 12 epoch\n",
    "fairseq-train $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --reset-lr-scheduler \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1 --skip-invalid-size-inputs-valid-test --tensorboard-logdir ./logs \n",
    "\n",
    "\n",
    "\n",
    "fairseq-train $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --reset-lr-scheduler \\\n",
    "    --lr-scheduler triangular --lr $BASE_LR --max-lr $MAX_LR \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1 --skip-invalid-size-inputs-valid-test --tensorboard-logdir ./logs "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitherbertpipenvf409fddaf3f446fd8dcf7490c441f6bd",
   "display_name": "Python 3.7.3 64-bit ('herbert': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}