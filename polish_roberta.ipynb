{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Herbert - Polish RoBERT'a model \n",
    "\n",
    "\n",
    "## Prepare the data.\n",
    "\n",
    "* Wikipedia, Link: \n",
    "* Oscar\n",
    "* Books, Polish \"Wolne lektury\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usefull resources\n",
    "\n",
    "\n",
    "* https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.pretraining.md\n",
    "* https://github.com/musixmatchresearch/umberto/issues/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "import\n"
    }
   ],
   "source": [
    "import csv\n",
    "import sys\n",
    "import datetime as dt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "#from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import mmap\n",
    "\n",
    "print(\"import\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary\n",
    "\n",
    "\n",
    "### Prepare data for vocab\n",
    "\n",
    "Create files for training vocabulary. One sentence per line, use polish sentence tokenizer. \n",
    "Sentence piece model is capable of handling around 12_000_000 sentences, so more sentences is not necessary.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the BPE sentencepiece model\n",
    "\n",
    "We used the [sentencepiece](https://github.com/google/sentencepiece)\n",
    "\n",
    "\n",
    "This can be done in two ways:\n",
    "- as a python module\n",
    "- as a command line \n",
    "\n",
    "\n",
    "If you want to use it as a commandline you should [build the C++ version from source](https://github.com/google/sentencepiece#c-from-source)\n",
    "\n",
    "\n",
    "Lower txt file \n",
    "\n",
    "```\n",
    "tr [:upper:] [:lower:] < corpus_books_wiki_12M_lines.txt > corpus_books_wiki_12M_lines_lower.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Train wikipedia vocab using commandline\n",
    "\n",
    "```\n",
    "spm_train \\\n",
    "    --input=./data/corpus_raw/corpus_books_wiki_12M_lines.txt \\\n",
    "    --max_sentence_length=4192\\\n",
    "    --model_prefix=./data/books_wikipedia_v32k_sen10M.spm.bpe \\\n",
    "    --vocab_size=32000 \\\n",
    "    --model_type=bpe \\\n",
    "    --shuffle_input_sentence=true \\\n",
    "    --input_sentence_size=10000000 \\\n",
    "    --bos_id=0 --eos_id=1 --pad_id=2 --unk_id=3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "spm_train \\\n",
    "    --input=corpus_books_wiki_12M_lines_lower.txt \\\n",
    "    --max_sentence_length=4192\\\n",
    "    --model_prefix=books_wikipedia_lower_v32k_sen10M.spm.bpe \\\n",
    "    --vocab_size=32000 \\\n",
    "    --model_type=bpe \\\n",
    "    --shuffle_input_sentence=true \\\n",
    "    --input_sentence_size=10000000 \\\n",
    "    --bos_id=0 --eos_id=1 --pad_id=2 --unk_id=3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--input=./data/corpus_wikipedia_2020-02-01_lines.txt --model_prefix=/home/ksopyla/dev/ermlab/herbert/data/wikipedia_upper_voc_32000_sen10000000 --model_type=bpe --num_threads=4 --vocab_size=32000  --input_sentence_size=10000000 --bos_id=0 --eos_id=1 --pad_id=2 --unk_id=3 --shuffle_input_sentence=true\n2020-02-27 13:11:27.769435\nVocab of 32000 tokens from ./data/corpus_wikipedia_2020-02-01_lines.txt create takes 0:13:14.934491\n"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "vocab_size = 32000\n",
    "model_type = \"bpe\"  \n",
    "iss = 10_000_000\n",
    "\n",
    "vocab_type = 'upper' \n",
    "#vocab_type = 'lower' \n",
    "\n",
    "\n",
    "tok_model = f\"wikipedia_{vocab_type}_voc_{vocab_size}_sen{iss}\"\n",
    "\n",
    "data_file = './data/corpus_wikipedia_2020-02-01_lines.txt'\n",
    "\n",
    "tok_model = os.path.abspath(f\"./data/{tok_model}\")\n",
    "\n",
    "piece_options = ' --bos_id=0 --eos_id=1 --pad_id=2 --unk_id=3 --shuffle_input_sentence=true'\n",
    "\n",
    "\n",
    "cmd = f\"--input={data_file} --model_prefix={tok_model} --model_type={model_type} --num_threads=4 --vocab_size={vocab_size}  --input_sentence_size={iss}\"+ piece_options\n",
    "print(cmd)\n",
    "\n",
    "start = dt.datetime.now()\n",
    "print(start)\n",
    "spm.SentencePieceTrainer.train(cmd)\n",
    "end = dt.datetime.now()\n",
    "\n",
    "print(f\"Vocab of {vocab_size} tokens from {data_file} create takes {end-start}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "32000\n['▁Będąc', '▁młodym', '▁programi', 'stą', '▁(', 'ho', 'ho', '),', '▁czy', 't', 'ałem', '▁\"', 'D', 'zia', 'dy', '\"', '▁w', '▁1983', 'r', '.']\n"
    }
   ],
   "source": [
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f\"{tok_model}.model\")\n",
    "\n",
    "# returns vocab size\n",
    "print(sp.get_piece_size())\n",
    "text = \"\"\"Będąc młodym programistą (hoho), czytałem \"Dziady\" w 1983r.\"\"\"\n",
    "\n",
    "# encode: text => id\n",
    "if vocab_type==\"lower\":\n",
    "    text = text.lower()\n",
    "\n",
    "print(sp.encode_as_pieces(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairseq vocab \n",
    "\n",
    "Change the bpe sentecepiece vocab separator form \\t to space. It is format needed by fairseq-preprocess \n",
    "Open vocab file and replce \\t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "vocab_file = './data/wiki_books_model/vocab/books_wikipedia_v32k_sen10M.spm.bpe.vocab'\n",
    "\n",
    "\n",
    "p = Path(vocab_file)\n",
    "\n",
    "output_path = f\"{p.with_suffix('')}_fair.vocab\"\n",
    "with open(output_path, 'w+') as output_file:\n",
    "    with open(vocab_file) as f:\n",
    "        \n",
    "        text = f.read().replace('\\t',' ')\n",
    "        output_file.write(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data with sentence piece model\n",
    "\n",
    "\n",
    "Encode wiki \n",
    "```\n",
    "# Encode Data with SentencePiece Tokenizer\n",
    "cd data/wiki_model\n",
    "\n",
    "for SPLIT in train valid test; do \\\n",
    "    spm_encode \\\n",
    "    --model=./vocab/wikipedia_upper_voc_32000_sen10000000.model \\\n",
    "    --extra_options=bos:eos \\\n",
    "    --output_format=piece \\\n",
    "    < corpus_wikipedia_2020-02-01_${SPLIT}.txt \\\n",
    "    > corpus_wikipedia_2020-02-01_${SPLIT}.txt.bpe\n",
    "done\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encode wiki books oscar**\n",
    "\n",
    "```\n",
    "# Encode Data with SentencePiece Tokenizer\n",
    "cd data/wiki_books_oscar_model\n",
    "\n",
    "for SPLIT in test train ; do \\\n",
    "    spm_encode \\\n",
    "    --model=./vocab/books_wikipedia_v32k_sen10M.spm.bpe.model \\\n",
    "    --extra_options=bos:eos \\\n",
    "    --output_format=piece \\\n",
    "    < corpus_wiki_books_oscar_${SPLIT}.txt \\\n",
    "    > corpus_wiki_books_oscar_${SPLIT}.txt.bpe\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data with fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\u001b[39m\u001b[1mInstalling \u001b[32m\u001b[1mfairseq\u001b[39m\u001b[22m...\u001b[39m\u001b[22m\n\u001b[K\u001b[39m\u001b[1mAdding\u001b[39m\u001b[22m \u001b[32m\u001b[1mfairseq\u001b[39m\u001b[22m \u001b[39m\u001b[1mto Pipfile's\u001b[39m\u001b[22m \u001b[31m\u001b[1m[packages]\u001b[39m\u001b[22m\u001b[39m\u001b[1m...\u001b[39m\u001b[22m\n\u001b[K\u001b[?25h✔ Installation Succeeded\u001b[0m \n\u001b[39m\u001b[1mInstalling dependencies from Pipfile.lock (d640ed)...\u001b[39m\u001b[22m\n\n\u001b[0m"
    }
   ],
   "source": [
    "\n",
    "!pipenv install fairseq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairseq  preprocess bpe encoded and splited data\n",
    "\n",
    "\n",
    "Wikipedia 2020-020-01 small(test) model\n",
    "\n",
    "```\n",
    "fairseq-preprocess \\\n",
    "    --only-source \\\n",
    "    --srcdict ./vocab/wikipedia_upper_voc_32000_sen10000000_fair.vocab \\\n",
    "    --trainpref corpus_wikipedia_2020-02-01_train.txt.bpe \\\n",
    "    --validpref corpus_wikipedia_2020-02-01_valid.txt.bpe \\\n",
    "    --testpref corpus_wikipedia_2020-02-01_test.txt.bpe \\\n",
    "    --destdir ./ \\\n",
    "    --workers 3\n",
    "```\n",
    "\n",
    "\n",
    "Wiki books oscar data\n",
    "\n",
    "Vocab32k process data\n",
    "\n",
    "```\n",
    "fairseq-preprocess \\\n",
    "    --only-source \\\n",
    "    --srcdict ./vocab/books_wikipedia_v32k_sen10M.spm.bpe_fair.vocab \\\n",
    "    --trainpref corpus_wiki_books_oscar_train_vocab32k.txt.bpe \\\n",
    "    --validpref corpus_wiki_books_oscar_test_vocab32k.txt.bpe \\\n",
    "    --destdir ./vocab32k \\\n",
    "    --workers 8\n",
    "```\n",
    "\n",
    "Vocab50k process data\n",
    "\n",
    "```\n",
    "fairseq-preprocess \\\n",
    "    --only-source \\\n",
    "    --srcdict ./vocab/books_wikipedia_v50k_sen10M.spm.bpe_fair.vocab \\\n",
    "    --trainpref corpus_wiki_books_oscar_train_vocab50k.txt.bpe \\\n",
    "    --validpref corpus_wiki_books_oscar_test_vocab50k.txt.bpe \\\n",
    "    --destdir ./vocab50k \\\n",
    "    --workers 8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model - small wikipedia\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS configuration for small wikipedia model\n",
    "\n",
    "#### p3 instance - linear decay\n",
    "\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=125000    # Total number of training steps\n",
    "WARMUP_UPDATES=10000    # Warmup the learning rate over this many updates\n",
    "PEAK_LR=0.0005          # Peak learning rate, adjust as needed\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=16          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=/mnt/efs/fs1/bert_model/datasets/roberta_data/wiki_model/\n",
    "SAVE_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_model\n",
    "LOGS_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_model/logs/\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1 --skip-invalid-size-inputs-valid-test \\\n",
    "    --save-dir $SAVE_DIR --tensorboard-logdir $LOGS_DIR --keep-last-epochs 10\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### g4 instance - wiki triangular\n",
    "\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=125000    # Total number of training steps\n",
    "STEP_SIZE=5000\n",
    "BASE_LR=0.0001\n",
    "PEAK_LR=0.001          # Peak learning rate, adjust as needed\n",
    "LR_SHRINK=0.6           # better is 0.9\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=16          # Increase the batch size 16x\n",
    "\n",
    "\n",
    "DATA_DIR=/mnt/efs/fs1/bert_model/datasets/roberta_data/wiki_model/\n",
    "SAVE_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_model_tri\n",
    "LOGS_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_model_tri/logs/\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler triangular --lr $BASE_LR --max-lr $PEAK_LR \\\n",
    "    --lr-period-updates $STEP_SIZE --lr-shrink $LR_SHRINK --shrink-min \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --skip-invalid-size-inputs-valid-test \\\n",
    "    --tensorboard-logdir $LOGS_DIR --log-format simple --log-interval 1  --save-dir $SAVE_DIR \\\n",
    "    --ddp-backend=no_c10d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### g4 instance - wiki cosine\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=30000    # Total number of training steps\n",
    "WARMUP_UPDATES=1000\n",
    "BASE_LR=0.00005\n",
    "PEAK_LR=0.0005          # Peak learning rate, adjust as needed\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=16          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=/mnt/efs/fs1/bert_model/datasets/roberta_data/wiki_model/\n",
    "SAVE_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_model_cos\n",
    "LOGS_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_model_cos/logs/\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler cosine --warmup-updates $WARMUP_UPDATES --lr-period-updates 1000 --t-mult 2 --lr-shrink 0.9 --lr $BASE_LR --max-lr $PEAK_LR \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --skip-invalid-size-inputs-valid-test \\\n",
    "    --tensorboard-logdir $LOGS_DIR --log-format simple --log-interval 1  --save-dir $SAVE_DIR\\\n",
    "    --ddp-backend=no_c10d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odra settings\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=30000    # Total number of training steps\n",
    "WARMUP_UPDATES=10000    # Warmup the learning rate over this many updates\n",
    "PEAK_LR=0.0005          # Peak learning rate, adjust as needed\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=6        \n",
    "UPDATE_FREQ=16          \n",
    "\n",
    "DATA_DIR=/mnt/hdd/goodwrite_nas/bert_model/datasets/roberta_data/wiki_model\n",
    "\n",
    "fairseq-train $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1 --skip-invalid-size-inputs-valid-test --tensorboard-logdir ./logs \n",
    "\n",
    "\n",
    "TOTAL_UPDATES=50000    # Total number of training steps\n",
    "WARMUP_UPDATES=5000\n",
    "PEAK_LR=0.001          # Peak learning rate, adjust as needed\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=6        \n",
    "UPDATE_FREQ=16          \n",
    "\n",
    "DATA_DIR=/mnt/hdd/goodwrite_nas/bert_model/datasets/roberta_data/wiki_model\n",
    "\n",
    "\n",
    "#after 32184 steps, 12 epoch\n",
    "fairseq-train $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --reset-lr-scheduler \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1 --skip-invalid-size-inputs-valid-test --tensorboard-logdir ./logs \n",
    "\n",
    "\n",
    "\n",
    "fairseq-train $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --reset-lr-scheduler \\\n",
    "    --lr-scheduler triangular --lr $BASE_LR --max-lr $MAX_LR \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1 --skip-invalid-size-inputs-valid-test --tensorboard-logdir ./logs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train wiki books oscar model \n",
    "\n",
    "Tech: \n",
    "\n",
    "* Pytorch 1.5\n",
    "* Apex \n",
    "* CUDA 10.2\n",
    "* fairseq 0.9\n",
    "\n",
    "\n",
    "### Experiment 1 - Linear decay\n",
    "\n",
    "\n",
    "Vocab: upper 32k tokens\n",
    "Train on AWS p3.16xlarge\n",
    "\n",
    "Efective batch size = MAX_SENTENCES*UPDATE_FREQ* num_gpu = 16\\*64\\*8 = 8192\n",
    "\n",
    "First start experiments with linear scheduler, run for 50k updates\n",
    "\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=50000    # Total number of training steps\n",
    "WARMUP_UPDATES=10000    # Warmup the learning rate over this many updates\n",
    "PEAK_LR=0.0005          # Peak learning rate, adjust as needed\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=64          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=/mnt/efs/fs1/bert_model/roberta_data/wiki_books_oscar/vocab32k/\n",
    "SAVE_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_books_oscar_32k_linear/\n",
    "LOGS_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_books_oscar_32k_linear/logs/\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1 --skip-invalid-size-inputs-valid-test \\\n",
    "    --save-dir $SAVE_DIR --tensorboard-logdir $LOGS_DIR --keep-last-epochs 10 \\\n",
    "    --ddp-backend=no_c10d \\\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tensorboard dev upload --logdir $LOGS_DIR \\\n",
    "    --name \"Herber polish Roberta model #1 - wiki, books, oscar 32k vocab32k\" \\\n",
    "    --description \"Wiki, books, oscar 32k, vocab 32k, upper, linear schedule \"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 - cyclic triangular\n",
    "\n",
    "Vocab: upper 32k tokens\n",
    "\n",
    "Train on AWS p3.16xlarge\n",
    "\n",
    "Cyclic triangular schedule, 5000 steps for rise to peek lr and fall to base lr, after each 5k stpe shirnk peak and base lr\n",
    "\n",
    "Efective batch size = MAX_SENTENCES*UPDATE_FREQ* num_gpu = 16\\*64\\*8 = 8192\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=50000    # Total number of training steps\n",
    "STEP_SIZE=5000\n",
    "BASE_LR=0.0001\n",
    "PEAK_LR=0.001          # Peak learning rate, adjust as needed\n",
    "LR_SHRINK=0.8           # \n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=64          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=/mnt/efs/fs1/bert_model/roberta_data/wiki_books_oscar/vocab32k/\n",
    "SAVE_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_books_oscar_32k_tri/\n",
    "LOGS_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_books_oscar_32k_tri/logs/\n",
    "\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler triangular --lr $BASE_LR --max-lr $PEAK_LR \\\n",
    "    --lr-period-updates $STEP_SIZE --lr-shrink $LR_SHRINK --shrink-min \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --skip-invalid-size-inputs-valid-test \\\n",
    "    --tensorboard-logdir $LOGS_DIR --log-format simple --log-interval 1  --save-dir $SAVE_DIR \\\n",
    "    --ddp-backend=no_c10d\n",
    "```\n",
    "\n",
    "```\n",
    "tensorboard dev upload --logdir $LOGS_DIR \\\n",
    "    --name \"Herber polish Roberta model #2 - wiki, books, oscar 32k vocab32k\" \\\n",
    "    --description \"Wiki, books, oscar 32k, vocab 32k, upper, tri schedule \"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3 - cyclic cosine\n",
    "\n",
    "Vocab: upper 32k tokens\n",
    "\n",
    "Train on AWS p3.16xlarge\n",
    "\n",
    "Cyclic cosine schedule, 5000 steps for rise to peek lr and fall to base lr, after each 5k stpe shirnk peak and base lr\n",
    "\n",
    "Efective batch size = MAX_SENTENCES*UPDATE_FREQ* num_gpu = 16\\*64\\*8 = 8192\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=50000    # Total number of training steps\n",
    "STEP_SIZE=2500\n",
    "WARMUP_UPDATES=2500 # same as triangular\n",
    "BASE_LR=0.0001\n",
    "PEAK_LR=0.001          # Peak learning rate, adjust as needed\n",
    "LR_SHRINK=0.8           # \n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=64          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=/mnt/efs/fs1/bert_model/roberta_data/wiki_books_oscar/vocab32k/\n",
    "SAVE_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_books_oscar_32k_cos1/\n",
    "LOGS_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_books_oscar_32k_cos1/logs/\n",
    "\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler cosine --lr $BASE_LR --max-lr $PEAK_LR \\\n",
    "    --warmup-updates $WARMUP_UPDATES  \\\n",
    "    --lr-period-updates $STEP_SIZE --t-mult 1 --lr-shrink $LR_SHRINK  \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --skip-invalid-size-inputs-valid-test \\\n",
    "    --tensorboard-logdir $LOGS_DIR --log-format simple --log-interval 1  --save-dir $SAVE_DIR \\\n",
    "    --ddp-backend=no_c10d\n",
    "```\n",
    "\n",
    "```\n",
    "tensorboard dev upload --logdir $LOGS_DIR \\\n",
    "    --name \"Herber polish Roberta model #3 - wiki, books, oscar 32k vocab32k\" \\\n",
    "    --description \"Wiki, books, oscar 32k, vocab 32k, upper, cosine schedule step=2500\"\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4 - cyclic cosine\n",
    "\n",
    "Vocab: upper 32k tokens\n",
    "\n",
    "Train on AWS p3.16xlarge\n",
    "\n",
    "Cyclic cosine schedule, 5000 steps for rise to peek lr and fall to base lr, after each 5k stpe shirnk peak and base lr\n",
    "Half the peak lr, because previous experiment not convergence\n",
    "\n",
    "\n",
    "Add num-workers to read data from disk\n",
    "\n",
    "\n",
    "Efective batch size = MAX_SENTENCES*UPDATE_FREQ* num_gpu = 16\\*64\\*8 = 8192\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=50000    # Total number of training steps\n",
    "STEP_SIZE=2500\n",
    "WARMUP_UPDATES=2500 # same as triangular\n",
    "BASE_LR=0.0001\n",
    "PEAK_LR=0.0005          # Peak learning rate, adjust as needed\n",
    "LR_SHRINK=0.8           # \n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=64          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=/mnt/efs/fs1/bert_model/roberta_data/wiki_books_oscar/vocab32k/\n",
    "SAVE_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_books_oscar_32k_cos1_2/\n",
    "LOGS_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_books_oscar_32k_cos1_2/logs/\n",
    "\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler cosine --lr $BASE_LR --max-lr $PEAK_LR \\\n",
    "    --warmup-updates $WARMUP_UPDATES  \\\n",
    "    --lr-period-updates $STEP_SIZE --t-mult 1  --lr-shrink $LR_SHRINK  \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --skip-invalid-size-inputs-valid-test \\\n",
    "    --tensorboard-logdir $LOGS_DIR --log-format simple --log-interval 1  --save-dir $SAVE_DIR \\\n",
    "    --ddp-backend=no_c10d --num-workers 2\n",
    "```\n",
    "\n",
    "```\n",
    "tensorboard dev upload --logdir $LOGS_DIR \\\n",
    "    --name \"Herber polish Roberta model #4 - wiki, books, oscar 32k vocab32k\" \\\n",
    "    --description \"Wiki, books, oscar 32k, vocab 32k, upper, cosine schedule step=2500 half lr=0.0005\"\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments 4, 5, 6 - cyclic cosine\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Vocab: upper 32k tokens\n",
    "\n",
    "Train on AWS p3.16xlarge\n",
    "\n",
    "Cyclic cosine schedule, 1000 steps for rise to peek lr and fall to base lr, \n",
    "\n",
    "\n",
    "Add num-workers to read data from disk\n",
    "\n",
    "\n",
    "Efective batch size = MAX_SENTENCES*UPDATE_FREQ* num_gpu = 16\\*64\\*8 = 8192\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=50000    # Total number of training steps\n",
    "STEP_SIZE=1000\n",
    "WARMUP_UPDATES=1000 \n",
    "BASE_LR=0.0001\n",
    "PEAK_LR=0.001          # Peak learning rate, adjust as needed\n",
    "LR_SHRINK=0.8           # \n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=64          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=/mnt/efs/fs1/bert_model/roberta_data/wiki_books_oscar/vocab32k/\n",
    "SAVE_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_books_oscar_32k_cos1_4/\n",
    "LOGS_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_books_oscar_32k_cos1_4/logs/\n",
    "\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.9 \\\n",
    "    --lr-scheduler cosine --lr $BASE_LR --max-lr $PEAK_LR \\\n",
    "    --warmup-updates $WARMUP_UPDATES  \\\n",
    "    --lr-period-updates $STEP_SIZE --t-mult 2  --lr-shrink $LR_SHRINK  \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --skip-invalid-size-inputs-valid-test \\\n",
    "    --tensorboard-logdir $LOGS_DIR --log-format simple --log-interval 1  --save-dir $SAVE_DIR \\\n",
    "    --ddp-backend=no_c10d --num-workers 2\n",
    "```\n",
    "\n",
    "```\n",
    "tensorboard dev upload --logdir $LOGS_DIR \\\n",
    "    --name \"Herber polish Roberta model #6 - wiki_books_oscar_32k_cos1_4\"  \\\n",
    "    --description \"Wiki, books, oscar 32k, vocab 32k, upper, cosine schedule step=1000 t-mult=2 clip-norm=0.9\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7 - Final tri vocab 32k - cyclic triangular\n",
    "\n",
    "Vocab: upper 32k tokens\n",
    "\n",
    "Train on AWS p3.16xlarge\n",
    "\n",
    "Efective batch size = MAX_SENTENCES*UPDATE_FREQ* num_gpu = 16\\*64\\*8 = 8192\n",
    "\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=125000    # Total number of training steps\n",
    "STEP_SIZE=5000\n",
    "BASE_LR=0.0001\n",
    "PEAK_LR=0.001          # Peak learning rate, adjust as needed\n",
    "LR_SHRINK=0.8           # \n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=64          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=/mnt/efs/fs1/bert_model/roberta_data/wiki_books_oscar/vocab32k/\n",
    "SAVE_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_books_oscar_32k_tri_full/\n",
    "LOGS_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_books_oscar_32k_tri_full/logs/\n",
    "\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler triangular --lr $BASE_LR --max-lr $PEAK_LR \\\n",
    "    --lr-period-updates $STEP_SIZE --lr-shrink $LR_SHRINK --shrink-min \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --skip-invalid-size-inputs-valid-test \\\n",
    "    --tensorboard-logdir $LOGS_DIR --log-format simple --log-interval 1  --save-dir $SAVE_DIR \\\n",
    "    --ddp-backend=no_c10d\n",
    "```\n",
    "\n",
    "```\n",
    "tensorboard dev upload --logdir $LOGS_DIR \\\n",
    "    --name \"Herber polish Roberta model #7 final tri - wiki, books, oscar 32k vocab32k\" \\\n",
    "    --description \"Wiki, books, oscar 32k, vocab 32k, upper, tri schedule \"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments 8 - cyclic cosine full\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Vocab: upper 32k tokens\n",
    "\n",
    "Train on AWS p3.16xlarge\n",
    "```\n",
    "TOTAL_UPDATES=125000    # Total number of training steps\n",
    "STEP_SIZE=1000\n",
    "WARMUP_UPDATES=5000 \n",
    "BASE_LR=0.00001\n",
    "PEAK_LR=0.0007          # Peak learning rate, adjust as needed\n",
    "LR_SHRINK=0.7           # \n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=64          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=/mnt/efs/fs1/bert_model/roberta_data/wiki_books_oscar/vocab32k/\n",
    "SAVE_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_books_oscar_32k_cos1_5/\n",
    "LOGS_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_books_oscar_32k_cos1_5/logs/\n",
    "\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.9 \\\n",
    "    --lr-scheduler cosine --lr $BASE_LR --max-lr $PEAK_LR \\\n",
    "    --warmup-updates $WARMUP_UPDATES  \\\n",
    "    --lr-period-updates $STEP_SIZE --t-mult 2  --lr-shrink $LR_SHRINK  \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --skip-invalid-size-inputs-valid-test \\\n",
    "    --tensorboard-logdir $LOGS_DIR --log-format simple --log-interval 1  --save-dir $SAVE_DIR \\\n",
    "    --ddp-backend=no_c10d --num-workers 2\n",
    "```\n",
    "\n",
    "```\n",
    "tensorboard dev upload --logdir $LOGS_DIR \\\n",
    "    --name \"Herber polish Roberta model #8 full 125k - wiki_books_oscar_32k_cos1_5\"  \\\n",
    "    --description \"Wiki, books, oscar 32k, vocab 32k, upper, cosine schedule  t-mult=2 clip-norm=0.9\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 9 - Linear full\n",
    "\n",
    "\n",
    "Vocab: upper 32k tokens\n",
    "Train on AWS p3.16xlarge\n",
    "\n",
    "Efective batch size = MAX_SENTENCES*UPDATE_FREQ* num_gpu = 16\\*64\\*8 = 8192\n",
    "\n",
    "First start experiments with linear scheduler, run for 50k updates\n",
    "\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=125000    # Total number of training steps\n",
    "WARMUP_UPDATES=10000    # Warmup the learning rate over this many updates\n",
    "PEAK_LR=0.001          # Peak learning rate, adjust as needed\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=64          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=/mnt/efs/fs1/bert_model/roberta_data/wiki_books_oscar/vocab32k/\n",
    "SAVE_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_books_oscar_32k_linear_full/\n",
    "LOGS_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_books_oscar_32k_linear_full/logs/\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.9 \\\n",
    "    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1 --skip-invalid-size-inputs-valid-test \\\n",
    "    --save-dir $SAVE_DIR --tensorboard-logdir $LOGS_DIR  \\\n",
    "    --ddp-backend=no_c10d \\\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "tensorboard dev upload --logdir $LOGS_DIR \\\n",
    "    --name \"Herber polish Roberta model #9 full 125k - wiki_books_oscar_32k_linear_full\"  \\\n",
    "    --description \"Wiki, books, oscar, vocab 32k, upper, linear clip-norm=0.9\"\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 10 - Linear 50k\n",
    "\n",
    "\n",
    "Vocab: upper 50k tokens\n",
    "Train on AWS p3.16xlarge\n",
    "\n",
    "Efective batch size = MAX_SENTENCES*UPDATE_FREQ* num_gpu = 16\\*64\\*8 = 8192\n",
    "\n",
    "First start experiments with linear scheduler, run for 50k updates\n",
    "\n",
    "\n",
    "```\n",
    "TOTAL_UPDATES=50000    # Total number of training steps\n",
    "WARMUP_UPDATES=10000    # Warmup the learning rate over this many updates\n",
    "PEAK_LR=0.001          # Peak learning rate, adjust as needed\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=64          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR=/mnt/efs/fs1/bert_model/roberta_data/wiki_books_oscar/vocab50k/\n",
    "SAVE_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_books_oscar_50k_linear50k/\n",
    "LOGS_DIR=/mnt/efs/fs1/bert_model/checkpoints/wiki_books_oscar_50k_linear50k/logs/\n",
    "\n",
    "fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.9 \\\n",
    "    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1 --skip-invalid-size-inputs-valid-test \\\n",
    "    --save-dir $SAVE_DIR --tensorboard-logdir $LOGS_DIR  \\\n",
    "    --ddp-backend=no_c10d \\\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "tensorboard dev upload --logdir $LOGS_DIR \\\n",
    "    --name \"Herber polish Roberta model #10 - wiki_books_oscar_50k_linear50K\"  \\\n",
    "    --description \"Wiki, books, oscar, vocab 50k, 50k steps, upper, linear clip-norm=0.9\"\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments (https://docs.google.com/spreadsheets/d/1fBhELqDB1kAxLCBvzeVM4OhqO4zx-meRUVljK1YZfF8/edit#gid=0)\n",
    "\n",
    "* experiment 1 - linear schedule peek_lr=5e-4  updates=50000, bsz=8192, test convergence speed of linear schedule, try to find optimal speed https://tensorboard.dev/experiment/pNCxXW9zQKKEoxkNN1LeKg/#scalars&tagFilter=lr%7C.*loss%24%7Cppl\n",
    "* experiment 2 - cyclic triangular schedule updates=50000, bsz=8192, cyclic step=5000 test convergence speed of linear schedule, try to find optimal speed https://tensorboard.dev/experiment/pNCxXW9zQKKEoxkNN1LeKg/#scalars&tagFilter=lr%7C.*loss%24%7Cppl\n",
    "* experiment 3 (wiki_books_oscar_32k_cos1) - cyclic cosine schedule,  updates=50000, bsz=8192, cyclic step=2500- test convergence speed of linear schedule, try to find optimal speed, should be similar to tirangular schedule in 5000 steps goes up and down with lr https://tensorboard.dev/experiment/SY64gY46SKq7wGohxgjlgg/#scalars&tagFilter=lr%7C.*loss%24%7Cppl after 23k steps experiment was stopped, loss jumped and plateau \n",
    "* experiment 4(wiki_books_oscar_32k_cos1_2) - similar to experiment 3 but peak learning rata was changed from 0.001 to 0.0005, I hope this will prevent loss jump and plateau  https://tensorboard.dev/experiment/wpK8EgmqQKiqJABKAndDwQ/\n",
    "* experiment 5 (wiki_books_oscar_32k_cos1_3) - similar to experiment 3, but decrese the step size and set t-mult to 2, we want restarts more frequenty https://tensorboard.dev/experiment/NzyYnXBQQ7qbBFYdu57byg/#scalars\n",
    "* experiment 6 (wiki_books_oscar_32k_cos1_4) - similar to experiment 5, but with clip-norm 0.9"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitherbertpipenvf409fddaf3f446fd8dcf7490c441f6bd",
   "display_name": "Python 3.7.3 64-bit ('herbert': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}